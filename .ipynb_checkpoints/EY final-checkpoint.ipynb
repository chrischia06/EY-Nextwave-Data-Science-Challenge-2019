{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "#read csv files\n",
    "df = pd.read_csv(\"data_train.csv\")\n",
    "df2 = pd.read_csv(\"data_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to predict whether the car will be in the city center, based on starting x,y coordinates, time_entry, time_exit.\n",
    "I can do this by:\n",
    "predicting a binary categorical variable - in city or not,\n",
    "predicting x,y coordinates\n",
    "\n",
    "Stratify by - time, hash_id, in city to begin with or not\n",
    "\n",
    "\n",
    "#Baseline for Final Entries 0.866589156610405\n",
    "\n",
    "#submission 1 - Baseline, (instationary_start = instationary_end)  = Logistic Regression \n",
    "#submission 2 - K-Nearest Neighbours, trained on in group, out group respectively\n",
    "#submission 3 - XGBoost on overall set\n",
    "#Submission 4 - XGoost, improvement by adding origin\n",
    "\n",
    "From data exploration - prior x,y coordinates are best predictors, decaying in relevance for each prior time entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_transform(df2):\n",
    "    #time - stratify into categories, e.g. hours\n",
    "    df = df2.copy()\n",
    "    df[[\"hour_entry\", \"minute_entry\", \"second_entry\"]] = df[\"time_entry\"].str.extract(r'(\\d+):(\\d+):(\\d+)').astype(int)\n",
    "    df[[\"hour_exit\", \"minute_exit\", \"second_exit\"]] = df[\"time_exit\"].str.extract(r'(\\d+):(\\d+):(\\d+)').astype(int)\n",
    "    df[\"t_nm1\"] = (3600 * df[\"hour_entry\"] + 60 * df[\"minute_entry\"] + df[\"second_entry\"]) \n",
    "    df[\"t_n\"] = (3600 * df[\"hour_exit\"] + 60 * df[\"minute_exit\"] + df[\"second_exit\"])\n",
    "    \n",
    "    [x_lower, x_upper, y_lower, y_upper] = [3750901.5068,3770901.5068, -19268905.6133,-19208905.6133]\n",
    "    df[\"in_entry\"] =  ((df[\"x_entry\"] >= x_lower) & (df[\"x_entry\"] <= x_upper) & \\\n",
    "                             (df[\"y_entry\"] >= y_lower) &  (df[\"y_entry\"] <= y_upper)) * 1\n",
    "    df[\"in_exit\"] =  ((df[\"x_exit\"] >= x_lower) & (df[\"x_exit\"] <= x_upper) & \\\n",
    "                             (df[\"y_exit\"] >= y_lower) &  (df[\"y_exit\"] <= y_upper)) * 1\n",
    "    \n",
    "    \n",
    "    #unknowns = ['x_exit','y_exit','dx','dy','vx','vy','dist','speed','n2','incenter_exit']\n",
    "    targets = df[df['hour_exit'] >= 15][['trajectory_id','x_exit','y_exit','in_exit']]\n",
    "    df.loc[df['hour_exit'] >= 15,['x_exit','y_exit','in_exit']] = np.nan\n",
    "\n",
    "    df[\"dt\"] = df[\"t_n\"] - df[\"t_nm1\"]\n",
    "   \n",
    "    df['n'] = df.groupby(['hash']).cumcount()\n",
    "    df['n2'] = df.groupby(['hash']).cumcount(ascending=False)\n",
    "    \n",
    "    unwanted = ['Unnamed: 0', 'Unnamed: 0.1','vmean','vmin','vmax','time_entry','time_exit',\n",
    "               'hour_entry','hour_exit','minute_entry','minute_exit','second_entry','second_exit']\n",
    "    return df.drop(unwanted,axis=1).rename(index=str,columns={\"trajectory_id\":\"id\"}), targets.rename(index=str,columns={\"trajectory_id\":\"id\"})\n",
    "\n",
    "#process data\n",
    "df_copy, targets = df_transform(df)\n",
    "\n",
    "df2_copy, _ = df_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def aggregate(df,df3):\n",
    "    df2 = df3.copy()\n",
    "    \n",
    "    #no need to aggregate for groups with just 1 entry\n",
    "    #hashgroup = df.groupby(['hash']).filter(lambda x: len(x) > 1).groupby(['hash'])\n",
    "    #hashgroup = df.groupby(['hash'])\n",
    "    #features = {}\n",
    "    #features['ratio_in'] = hashgroup[['in_entry','in_exit']].apply(np.nanmean)\n",
    "    #features['avg_x'] = hashgroup[['x_entry','x_exit']].apply(np.nanmean)\n",
    "    #features['avg_y'] = hashgroup[['y_entry','y_exit']].apply(np.nanmean)\n",
    "    #features['total_dist'] = hashgroup['dist'].sum()\n",
    "    \n",
    "    #for f in features:\n",
    "    #    df2 = df2.merge(features[f].rename(f),left_on=\"hash\",right_index=True,how='outer')\n",
    "        \n",
    "   #mean, median, max, min of x, y positions\n",
    "`\n",
    "    #position at origin\n",
    "    f = ['hash','x_entry','y_entry','t_nm1','x_exit','y_exit']\n",
    "    fs = df.loc[df['n'] == 0][f]\n",
    "    fs.columns = ['hash','x_0','y_0','t_0','x_1','y_1']\n",
    "    df2 = df2.merge(fs,left_on=\"hash\",right_on=\"hash\",how='inner')\n",
    "    df2[\"sum_dt_nm1\"] = df2[\"t_nm1\"] - df2['t_0']  \n",
    "    #df2['avg_speed'] = df2['total_dist'].divide(df2['sum_dt_nm1'])\n",
    "    df2.loc[df2['x_1'].isna() == True,'x_1'] = df2.loc[df2['x_1'].isna() == True,'x_0']\n",
    "    df2.loc[df2['y_1'].isna() == True,'y_1'] = df2.loc[df2['y_1'].isna() == True,'y_0']\n",
    "    \n",
    "    #prior 2 positions\n",
    "    f = ['hash',\"x_exit\",\"y_exit\",\"x_entry\",\"y_entry\",\"t_nm1\",'t_n']#,\n",
    "         #'dx','dy','dist','dt','vx','vy','speed'\n",
    "    fs = df.loc[df['n2'] == 1][f]\n",
    "    fs.columns = ['hash',\"x_nm2\",\"y_nm2\",\"x_nm3\",\"y_nm3\",\"t_nm3\",'t_nm2']#,\n",
    "         #'dx_nm2','dy_nm2','dist_nm2','dt_nm2','vx_nm2','vy_nm2','speed_nm2']\n",
    "    \n",
    "    df2 = df2.merge(fs,left_on=\"hash\",right_on=\"hash\",how='outer')\n",
    "    \n",
    "    #impute missing values\n",
    "    #df2.loc[df2['x_nm2'].isna(),'x_nm2'] = df2.loc[df2['x_nm2'].isna(),'x_0']\n",
    "    #df2.loc[df2['y_nm2'].isna(),'y_nm2'] = df2.loc[df2['y_nm2'].isna(),'y_0']\n",
    "    #df2.loc[df2['x_nm3'].isna(),'x_nm3'] = df2.loc[df2['x_nm3'].isna(),'x_0']\n",
    "    #df2.loc[df2['y_nm3'].isna(),'y_nm3'] = df2.loc[df2['y_nm3'].isna(),'y_0']\n",
    "    #df2.loc[df2['t_nm3'].isna(),'t_nm3'] = df2.loc[df2['t_nm3'].isna(),'t_nm1']\n",
    "    #df2.loc[df2['t_nm2'].isna(),'t_nm2'] = df2.loc[df2['t_nm2'].isna(),'t_nm1']\n",
    "                                                              \n",
    "            \n",
    "    #df2['dt_nm1'] = df2['t_m1'] - df2['t_nm2']\n",
    "    df2['dx_nm1'] = df2['x_entry'] - df2['x_nm2']\n",
    "    df2['dy_nm1'] = df2['y_entry'] - df2['y_nm2']\n",
    "    df2['dx_nm2'] = df2['x_nm2'] - df2['x_nm3']\n",
    "    df2['dy_nm2'] = df2['y_nm2'] - df2['y_nm3']\n",
    "    #df2['dist_nm1'] = np.sqrt(df2['dx_nm1'] * df2['dx_nm1'] + df2['dy_nm1'] * df2['dy_nm1'])\n",
    "    #df2['vx_nm1'] = df2['dx_nm1'].divide(df2['dt_nm1'])\n",
    "    #df2['vy_nm1'] = df2['dy_nm1'].divide(df2['dt_nm1'])\n",
    "    #df2['speed_n'] = df2['dist_n'].divide(df2['dt_nm1'])\n",
    "\n",
    "    return df2\n",
    "\n",
    "\n",
    "final = df_copy[df_copy['t_n'] >= 15*60*60]\n",
    "unknowns = ['x_exit','y_exit','n2','in_exit','dx','dy','vx','vy','dist','speed']\n",
    "final = final.drop(unknowns,axis=1)\n",
    "final = aggregate(df_copy,final)\n",
    "final = final.set_index('id')\n",
    "final_tn0 = final[final['dt'] > 0]\n",
    "\n",
    "            \n",
    "final2 = df2_copy[df2_copy['t_n'] >= 15*60*60]\n",
    "final2 = final2.drop(unknowns,axis=1)\n",
    "final2 = aggregate(df2_copy,final2)\n",
    "final2 = final2.set_index('id')\n",
    "final2_tn0 = final2[final2['dt'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline overall - f1: 0.8876970163859333\n",
    "#f1_score(df_copy[\"incenter_entry\"],df_copy[\"incenter_exit\"])\n",
    "#pd.crosstab(df_copy[\"incenter_entry\"],df_copy[\"incenter_exit\"])\n",
    "#classes imbalanced, out : in = 2.348 : 1 \n",
    "#challenge is to predict those that move from inside to outside\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Baseline for final  - 0.866589156610405\n",
    "#baseline for final_tn0 - 0.7211841921519341, 0.7424115867620404 with XGBoost\n",
    "#f1_score(final[\"incenter_entry\"],final[\"incenter_exit\"])\n",
    "#pd.crosstab(final[\"incenter_entry\"],final[\"incenter_exit\"])\n",
    "#Class imbalance rises to 2.7 : 1\n",
    "\n",
    "\n",
    "#Using XGBOOST\n",
    "#features = ['x_entry', 'y_entry','dt','totalsecs_entry', 'incenter_entry'] Best initial features on overall set\n",
    "#improvement to 0.8732285592178957 by training on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Feature Selection; XGboost\n",
    "import xgboost as xgb\n",
    "\n",
    "bestfeatures = ['x_entry','y_entry','x_nm2','x_nm3','y_nm2','y_nm3',\n",
    "                'x_1','y_1','x_0','y_0',\n",
    "                'dt','sum_dt_nm1','in_entry',\n",
    "                ] #+ ['dx_nm1','dy_nm1','t_nm1','dx_nm2','dy_nm2']\n",
    "\n",
    "data = pd.merge(targets,final_tn0,left_index=True,right_index=True,how='inner')\n",
    "xgbc=xgb.XGBClassifier(n_estimators=100,\n",
    "                         max_depth=8,\n",
    "                         learning_rate=0.2,\n",
    "                         gamma=0.3,\n",
    "                         early_stopping_rounds=5,\n",
    "                      scale_pos_weights = 2.5) \n",
    "print(cross_val_score(xgbc,data[bestfeatures],data[\"in_exit\"],cv=2,scoring=\"f1\").mean())\n",
    "\n",
    "xgbc.fit(data[bestfeatures],data[\"in_exit\"])\n",
    "xgb.plot_importance(xgbc)\n",
    "pd.crosstab(data['in_exit'],xgbc.predict(data[bestfeatures]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(df2, df3):\n",
    "    df = df2.copy()\n",
    "    whole = df3[df3['dt'] > 0]\n",
    "    \n",
    "    df[\"norm_dt\"] = (df[\"dt\"] - df[\"dt\"].mean()) / df[\"dt\"].std()\n",
    "    df[\"minmax_dt\"] = 2 * (df[\"dt\"] - df[\"dt\"].min()) / (df[\"dt\"].max() - df[\"dt\"].min()) - 1\n",
    "    df['norm_in_entry'] = 2 * df2['in_entry'] - 1\n",
    "\n",
    "    #Feature Selection; XGboost\n",
    "\n",
    "    xs = ['x_entry','x_nm2','x_nm3','x_0','x_1']\n",
    "    ys = ['y_entry','y_nm2','y_nm3','y_0','y_1']\n",
    "    \n",
    "    \n",
    "    mu_x = np.nanmean(np.array(whole[['x_entry','x_exit']]).reshape(-1,1))\n",
    "    std_x = np.nanstd(np.array(whole[['x_entry','x_exit']]).reshape(-1,1))\n",
    "    mu_y = np.nanmean(np.array(whole[['y_entry','y_exit']]).reshape(-1,1))\n",
    "    std_y = np.nanstd(np.array(whole[['y_entry','y_exit']]).reshape(-1,1))\n",
    "    \n",
    "    \n",
    "    for x in xs:\n",
    "        df['norm_'+str(x)] = (df[x] - df['x_entry'].mean()) / df['x_entry'].std()\n",
    "    for y in ys:\n",
    "        df['norm_'+str(y)] = (df[y] - df['y_entry'].mean()) / df['y_entry'].std()\n",
    "    \n",
    "    df['norm_dx_nm1'] = (df['dx_nm1'] / df['x_entry'].std())\n",
    "    df['norm_dy_nm1'] = (df['dy_nm2'] / df['y_entry'].std())\n",
    "    df['norm_dx_nm2'] = (df['dx_nm1'] / df['x_entry'].std())\n",
    "    df['norm_dy_nm2'] = (df['dy_nm2'] / df['y_entry'].std())\n",
    "    \n",
    "    return df\n",
    "\n",
    "test = normalise(final_tn0,df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier ,RadiusNeighborsClassifier\n",
    "\n",
    "#cross val #0.8752232230385206\n",
    "test = normalise(final_tn0,df_copy)\n",
    "\n",
    "#bestfeatures = ['x_entry','y_entry','x_nm2','x_nm3','y_nm2','y_nm3',\n",
    "#                'x_1','y_1','x_0','y_0',\n",
    "#                'dt','in_entry',\n",
    "#                ]\n",
    "\n",
    "#bestfeatures = ['x_entry','y_entry','in_entry','dt','x_nm2','y_nm2','x_nm3','y_nm3','x_1','y_1','x_0','y_0']\n",
    "`\n",
    "bestfeatures = ['x_entry','y_entry','x_nm2','y_nm2','x_0', 'y_0','x_1','y_1','dt','in_entry`']\n",
    "norm_features = ['norm_' + x for x in bestfeatures] \n",
    "\n",
    "#\n",
    "\n",
    "#data = pd.merge(test,targets,left_index=True,right_index=True,how='inner')\n",
    "#print(cross_val_score(mlpc,data[bestfeatures],data[\"in_exit\"],cv=2,scoring=\"f1\").mean())\n",
    "\n",
    "\n",
    "\n",
    "data = pd.merge(test,targets,left_index=True,right_index=True,how='inner')\n",
    "data_in =  data[data['in_entry'] == True]\n",
    "data_out =  data[data['in_entry'] == False]\n",
    "mlpc = MLPClassifier(warm_start=True,solver='adam',alpha=0.5,early_stopping = True, activation='relu',max_iter=200)\n",
    "print(cross_val_score(mlpc,data[norm_features],data[\"in_exit\"],cv=2,scoring=\"f1\").mean())\n",
    "#knn = KNeighborsClassifier(p=1,n_neighbors=50,weights='uniform')\n",
    "#knn = RadiusNeighborsClassifier()\n",
    "#print(cross_val_score(knn,data_in[norm_features],data_in[\"in_exit\"],cv=2,scoring=\"f1\").mean())\n",
    "\n",
    "\n",
    "#for traj>1\n",
    "#0.8846013995893918 n-neighbors = 8\n",
    "#Best features: Best params - p = 1, manhatten distance, weights = distance\n",
    "#norm_features = [\"norm_xentry\",\"norm_yentry\",'norm_timedelta','incenter_start','norm_xmean','norm_ymean',\n",
    "#                 'norm_xorigin','norm_yorigin','norm_xmedian','norm_ymedian']\n",
    "\n",
    "\n",
    "\n",
    "#Best features for those with 1 traj, \n",
    "#norm_features = [\"norm_xentry\",\"norm_yentry\",'norm_timedelta','incenter_start']\n",
    "#params - n-neighbors = 8, p=1, weights= distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(cross_val_score(knn_final,norm_df[norm_features],norm_df[\"incenter_end\"],cv=2,scoring=\"f1\").mean())\n",
    "#knn_final.fit(norm_final[norm_features],norm_final[\"incenter_end\"])\n",
    "#norm_final[\"knn\"] = norm_final[norm_features]\n",
    "\n",
    "\n",
    "# Final - 0.7525773195876289\n",
    "#from sklearn.svm import LinearSVC\n",
    "#svc = SVC(gamma='auto')\n",
    "#print(cross_val_score(svc,data[bestfeatures],data[\"in_exit\"],cv=2,scoring=\"f1\").mean())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(data_in['in_exit'],data_in['in_entry']))\n",
    "pd.crosstab(data_out['in_exit'],data_out['in_entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['in_entry','in_exit'])['norm_dx_nm2'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "gpc = GaussianProcessClassifier()\n",
    "\n",
    "bestfeatures = ['in_entry']\n",
    "norm_features = ['norm_' + x for x in bestfeatures]\n",
    "\n",
    "\n",
    "\n",
    "#Final -0.7650575946895743\n",
    "print(cross_val_score(gpc,data_in[norm_features],data_in[\"in_exit\"],cv=2,scoring=\"f1\").mean())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf.keras.models import Sequential()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(,1)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_lower, x_upper, y_lower, y_upper] = [3750901.5068,3770901.5068, -19268905.6133,-19208905.6133]\n",
    "\n",
    "    \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "lasso = ElasticNet()\n",
    "kr = KernelRidge()\n",
    "#rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "#mor = MultiOutputRegressor(lr)\n",
    "dp = DotProduct()\n",
    "\n",
    "mor = MultiOutputRegressor(lr)\n",
    "\n",
    "\n",
    "X = test\n",
    "\n",
    "bestfeatures = ['x_entry','y_entry','x_nm2','y_nm2','x_0', 'y_0','x_1','y_1','dt','in_entry']\n",
    "norm_features = ['norm_' + x for x in bestfeatures] \n",
    "\n",
    "print(cross_val_score(mor,data_in[bestfeatures],data_in[['x_exit','y_exit']], cv=2).mean())\n",
    "print(cross_val_score(mor,data_out[bestfeatures],data_out[['x_exit','y_exit']], cv=2).mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#mor.fit(X[bestfeatures], X['x_exit','y_exit'])\n",
    "#x = pd.DataFrame(mor.predict(X[[\"norm_yentry\",\"norm_xentry\",\"norm_timedelta\",\"norm_totalsecsentry\"]]))\n",
    "#x.columns=['x_preds','y_preds']\n",
    "#X = X.reset_index()\n",
    "#X[['x_preds','y_preds']] = x\n",
    "#X[['x_preds']] *=  df_copy[\"x_entry\"].std()\n",
    "#X[['x_preds']] +=  df_copy[\"x_entry\"].mean()\n",
    "#X[['y_preds']] *=  df_copy[\"y_entry\"].std()\n",
    "#X[['y_preds']] +=  df_copy[\"y_entry\"].mean()\n",
    "\n",
    "\n",
    "#X['target'] = (X[\"x_preds\"] >= x_lower) & (X[\"x_preds\"] <= x_upper) & (X[\"y_preds\"] >= y_lower) & (X[\"y_preds\"] <= y_upper)\n",
    "\n",
    "#f1_score(X['target'], X['incenter_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2.loc[final2['dt'] > 0,\"target\"] = xgbc.predict(final2_tn0[bestfeatures])\n",
    "final2.loc[final2['dt'] == 0,\"target\"] = final2.loc[final2['dt'] == 0,\"in_entry\"]\n",
    "#X_pred = output.values.reshape(-1,10)\n",
    "#output[\"target\"] = knn.predict(X_pred)\n",
    "#output[['trajectory_id','target']]\n",
    "final2['target'].to_csv(\"output.csv\",index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final2['target'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
